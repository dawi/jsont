// Package jsont provides a non validating json tokenizer that also emits insignificant tokens (like whitespace).
//
// Example
//
// The following json:
//   { "Hello" : "World" }
//
// Will be split into the following tokens:
//   |{| |"Hello"| |:| |"World"| |}|
//
// Usage
//
// Usage example:
//
// 	tokenizer := jsont.NewTokenizer(strings.NewReader(jsonInput))
//
// 	for tokenizer.Next() {
// 		token := tokenizer.Token()
// 		fmt.Println(token.Type)
// 		fmt.Println(token.Value)
// 	}
//
// 	if tokenizer.Error() != nil {
// 		fmt.Println(tokenizer.Error())
// 	}
//
// Disclaimer
//
// The behavior of this tokenizer is rarely useful.
// In fact only consider using it, if you are interested in whitespace tokens  and don't need to check if the given input is valid json.
//
package jsont

import (
	"bufio"
	"io"
)

type Tokenizer interface {

	// Scan advances the Scanner to the next token, which will then be available
	// through the Token method. It returns false if no more Tokens are available,
	// either because of reaching the end of the input or an error.
	Next() bool

	// Token returns the most recent token generated by a call to Next.
	Token() Token

	// Error returns the first non-EOF error that was encountered by the Tokenizer.
	Error() error
}

func NewTokenizer(reader io.Reader) Tokenizer {
	return &tokenizer{bufio.NewReader(reader), nil, []Token{{}}}
}

type tokenizer struct {
	reader *bufio.Reader
	error  error
	buffer []Token
}

func (t *tokenizer) Next() bool {
	t.fillBuffer()
	t.advance()
	t.lookAhead()
	return t.buffer[0].Type != Empty
}

func (t *tokenizer) Token() Token {
	return t.buffer[0]
}

func (t *tokenizer) Error() error {
	if t.error != nil && t.error != io.EOF && len(t.buffer) == 1 {
		return t.error
	}
	return nil
}

func (t *tokenizer) fillBuffer() {
	if t.error == nil {
		toRead := 4 - len(t.buffer)
		for i := 0; i < toRead; i++ {
			token, err := readToken(t.reader)
			if token.Type != Empty {
				t.buffer = append(t.buffer, token)
			}
			if err != nil {
				t.error = err
				t.buffer = append(t.buffer, Token{})
				break
			}
		}
	}
}

func (t *tokenizer) advance() {
	if len(t.buffer) > 1 {
		t.buffer = t.buffer[1:]
	}
}

func (t *tokenizer) lookAhead() {
	if t.buffer[0].Type == String {
		token1 := t.getToken(1)
		token2 := t.getToken(2)
		if token1.Type == Colon || token2.Type == Colon {
			t.buffer[0].Type = FieldName
		}
	}
}

func (t *tokenizer) getToken(i int) Token {
	if i < len(t.buffer) {
		return t.buffer[i]
	}
	return Token{}
}
